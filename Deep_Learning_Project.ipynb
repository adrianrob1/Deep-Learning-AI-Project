{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Used for Colab 'Run all' command and Wandb experiment name:\n",
        "- CIFAR or MNIST: CIFAR10 vs MNIST dataset (loaded using code)\n",
        "- FC or CNN: MLP vs CNN model (created through code)"
      ],
      "metadata": {
        "id": "uxhqw22Nfmjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = 'CIFAR-FC-SparseVsDense'"
      ],
      "metadata": {
        "id": "pA9dW5u7dZRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1PLX_INa2Qx"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Installs Pytorch Lightning and Wandb on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKMD8ThCT11e"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWba7jeAtXKd"
      },
      "outputs": [],
      "source": [
        "# @title Import dependencies\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT88cI_-t7N0"
      },
      "outputs": [],
      "source": [
        "# @title Reproducibility stuff\n",
        "\n",
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(0)\n",
        "\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True  # Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGgg6q15i3bW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e93945-cfea-45a6-9d7c-fb2643c93aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrianrob\u001b[0m (\u001b[33msapienza-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        " # @title Setup Wandb\n",
        "\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2-boU_uoRdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d660be-112f-4b8e-8693-c6cd129751c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {DEVICE}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cz23_ZVl5UM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b0f006f9-0f2b-4f42-c04a-316b9433d75e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20230121_134341-amlenedw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense/runs/amlenedw\" target=\"_blank\">misty-dragon-43</a></strong> to <a href=\"https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense\" target=\"_blank\">https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense/runs/amlenedw\" target=\"_blank\">https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense/runs/amlenedw</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb_logger = WandbLogger(project=PROJECT_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base Model"
      ],
      "metadata": {
        "id": "Y03O7MGKisUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # in lightning, forward defines the prediction/inference actions\n",
        "        res = self.model(x)\n",
        "        return res\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop. It is independent of forward\n",
        "        preds, loss, acc, ts = self._get_preds_metrics(batch)\n",
        "        # logging\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_acc\", acc)\n",
        "        self.log(\"train_time_step\", ts)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # validation_step defines the validation loop\n",
        "        preds, loss, acc, ts = self._get_preds_metrics(batch)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.log(\"val_acc\", acc)\n",
        "        self.log(\"val_time_step\", ts)\n",
        "        return preds\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # test defines the test loop\n",
        "        preds, loss, acc, ts = self._get_preds_metrics(batch)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_acc\", acc)\n",
        "        self.log(\"test_time_step\", ts)\n",
        "        return loss\n",
        "    \n",
        "    def _get_preds_metrics(self, batch):\n",
        "        # time step (ms) calculation: https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964/6\n",
        "        from torchmetrics.functional import accuracy\n",
        "\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start.record()\n",
        "\n",
        "        x, y = batch\n",
        "        xd, yd = x.to(DEVICE), y.to(DEVICE)\n",
        "        \n",
        "        preds = self(xd)\n",
        "\n",
        "        end.record()\n",
        "\n",
        "        # Waits for everything to finish running\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        time_step = start.elapsed_time(end)\n",
        "        loss = F.cross_entropy(preds, yd)\n",
        "        acc = accuracy(preds, yd, self.task, num_classes=self.output_size)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "        return preds, loss, acc, time_step\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), 1e-3)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "tuPhgaw9iwDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL4nVwstwTWD"
      },
      "source": [
        "## MNIST Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-3nkeCtuyPZ"
      },
      "outputs": [],
      "source": [
        "if(\"mnist\" in PROJECT_NAME.lower()):\n",
        "  print(\"loading mnist dataset\")\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "  train_set = MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "  test_set = MNIST('data', train=False,\n",
        "                    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "  # use 20% of training data for validation\n",
        "  train_set_size = int(len(train_set) * 0.8)\n",
        "  valid_set_size = len(train_set) - train_set_size\n",
        "\n",
        "  batch_size = 64\n",
        "\n",
        "  train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size])\n",
        "\n",
        "  train_set = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "  val_set = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
        "  test_set = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  classes = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3zM3cpQwdWc"
      },
      "source": [
        "## CIFAR10 Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhMV0uW3who3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f6507f-4879-464d-bb3c-498dfcdb852c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading cifar dataset\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "if(\"cifar\" in PROJECT_NAME.lower()):\n",
        "  print(\"loading cifar dataset\")\n",
        "  # Source: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "  transform = transforms.Compose(\n",
        "      [transforms.Resize((28,28)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  batch_size = 64\n",
        "\n",
        "  train_set = CIFAR10(root='./data', train=True,\n",
        "                                          download=True, transform=transform)\n",
        "  test_set = CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "  \n",
        "  # use 20% of training data for validation\n",
        "  train_set_size = int(len(train_set) * 0.8)\n",
        "  valid_set_size = len(train_set) - train_set_size\n",
        "\n",
        "  train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size])\n",
        "\n",
        "  train_set = DataLoader(train_set, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=1)\n",
        "  val_set = DataLoader(valid_set, batch_size=batch_size,\n",
        "                                            shuffle=False, num_workers=1)\n",
        "  test_set = DataLoader(test_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=1)\n",
        "\n",
        "  classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Q8VYwTwZlZ"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M5jarIOTHUN"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import Callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(monitor='val_acc', mode='max')\n",
        " \n",
        "class LogPredictionsCallback(Callback):\n",
        "    '''\n",
        "    Wandb logging\n",
        "    '''\n",
        "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "        t = torch.cuda.get_device_properties(0).total_memory\n",
        "        r = torch.cuda.memory_reserved(0)\n",
        "        a = torch.cuda.memory_allocated(0)\n",
        "\n",
        "        wandb_logger.log_metrics({'Train_Total_VRAM': t,\n",
        "                'Train_Reserved_VRAM': r,\n",
        "                'Train_Allocated_VRAM': a,\n",
        "                'Train_Free_VRAM': r-a})\n",
        "\n",
        "    def on_validation_batch_end(\n",
        "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
        "        if batch_idx == 0:\n",
        "            n = 20\n",
        "            x, y = batch\n",
        "            images = [img for img in x[:n]]\n",
        "\n",
        "            if classes is not None:\n",
        "              captions = [f'Ground Truth: {classes[y_i]} - Prediction: {classes[y_pred]}' for y_i, y_pred in zip(y[:n], outputs[:n])]\n",
        "            else:\n",
        "              captions = [f'Ground Truth: {y_i} - Prediction: {y_pred}' for y_i, y_pred in zip(y[:n], outputs[:n])]\n",
        "            \n",
        "            # Option 1: log images with `WandbLogger.log_image`\n",
        "            wandb_logger.log_image(key='sample_images', images=images, caption=captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX8-68XU5Nfp"
      },
      "source": [
        "## Random Projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHR3JkJR5Rbd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Generates N x M matrix (gaussian random projections)\n",
        "  \n",
        "  @ortho: if true the returned matrix is orthogonal\n",
        "'''\n",
        "def generate_gaussian_rp(N, M, ortho=False):\n",
        "  '''\n",
        "  We want to generate a NxM matrix by usign a gaussian distribution\n",
        "  Can also orthogonalize the matrix using QR decomposition\n",
        "  '''\n",
        "  if(ortho): # using the gaussian distribution we get an approximate orthogonal matrix, so orthogonalization may not be necessary\n",
        "    rp = torch.randn(N, M, device=DEVICE)\n",
        "    return torch.linalg.qr(rp)[0] # QR decomposition where Q is an orthogonal matrix\n",
        "  else:\n",
        "    return torch.randn(N, M, device=DEVICE) / math.sqrt(M)\n",
        "\n",
        "'''\n",
        "  Generates n_components x n_features matrix (sparse and very sparse random projections)\n",
        "\n",
        "  @original: if true uses code from bit.ly/3ZOt9S0, otherwise we adjust by swapping d (subspace) and D (full space)\n",
        "  @fullRange: if false values are either 0 or 1 before multiplication by np.sqrt(density), otherwise they are -1, 0 or 1\n",
        "  @variation: sparse2 type which uses sqrt of D (full space) as elements p_ij in the projection matrix\n",
        "  @sparse_type:\n",
        "    - None: dense representation w/ dense operations\n",
        "    - coo: sparse coo representation w/ sparse ops\n",
        "    - csr: sparse csr representation w/ sprase ops\n",
        "    - csc: sparse csc representation w/ sparse ops\n",
        "'''\n",
        "def generate_sparse_rp(n_features, n_components, original=False, sparse_type=None, fullRange=True, variation=False):\n",
        "  '''\n",
        "  Sparse Random Projection algorithm (Dimitris Achlioptas)\n",
        "  We want to generate a NxM matrix by first having all 0s, then pick some (row, col) elements and assign 1\n",
        "  After this, we can multiply by a constant value which will compensate for the fact that we have mostly 0s\n",
        "  The resulting matrix should approximate an orthogonal matrix for a smoother energy landscape (subspace)\n",
        "\n",
        "  Very Sparse Random Projections (Li et al.): we can reduce the density of the matrix even more\n",
        "  '''\n",
        "\n",
        "  from sklearn.utils.random import sample_without_replacement\n",
        "\n",
        "  eps = 0.1\n",
        "  denominator = (eps**2 / 2) - (eps**3 / 3)\n",
        "  johnson_lindenstrauss_min_dim = (4 * np.log(n_components) / denominator).astype(np.int64)\n",
        "\n",
        "  print(\"min dim should be: \", johnson_lindenstrauss_min_dim)\n",
        "\n",
        "  density = 1 / np.sqrt(n_features) if original else 1 / np.sqrt(n_components)\n",
        "\n",
        "  if density == 1:\n",
        "    # skip index generation if totally dense\n",
        "    binomial = torch.distributions.Binomial(total_count=1, probs=0.5)\n",
        "    if(fullRange):\n",
        "      components = binomial.sample((n_components, n_features)) * 2 - 1\n",
        "    else:\n",
        "      components = binomial.sample((n_components, n_features))\n",
        "    components = 1 / np.sqrt(n_components) * components\n",
        "\n",
        "  else:\n",
        "    if(sparse_type != None):\n",
        "      col_idx = torch.tensor([], dtype=torch.long, device=DEVICE)\n",
        "      row_idx = torch.tensor([], dtype=torch.long, device=DEVICE)\n",
        "      values = torch.tensor([], device=DEVICE)\n",
        "\n",
        "    components = torch.zeros((n_components, n_features), dtype=torch.float, device=DEVICE)\n",
        "    for i in range(n_components):\n",
        "        # find the indices of the non-zero components for row i\n",
        "        nnz_idx = torch.distributions.Binomial(total_count=n_features, probs=density).sample()\n",
        "        # get nnz_idx column indices\n",
        "        c_idx = torch.tensor(\n",
        "            sample_without_replacement(\n",
        "                n_population=n_features, n_samples=nnz_idx, random_state=42\n",
        "            ),\n",
        "            dtype=torch.long,\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "        if(fullRange):\n",
        "          data = torch.distributions.Binomial(total_count=1, probs=0.5).sample(sample_shape=c_idx.size()).to(DEVICE) * 2 - 1 # row with values -1 or 1\n",
        "        else:\n",
        "          data = torch.distributions.Binomial(total_count=1, probs=0.5).sample(sample_shape=c_idx.size()).to(DEVICE) # row with values 0 or 1\n",
        "        \n",
        "        # assign data only to those columns\n",
        "        if(sparse_type == None):\n",
        "          components[i, c_idx] = data.float()\n",
        "        else:\n",
        "          # for sparse representations we first get coo represent. then convert it to the other types\n",
        "          row_idx = torch.cat([row_idx, torch.ones(c_idx.shape[0], dtype=torch.long, device=DEVICE) * i], dim=0)\n",
        "          col_idx = torch.cat([col_idx, c_idx], dim=0)\n",
        "          \n",
        "          if(variation): # pytorch doesn't support sparse multiplication for scalars so we have to do it here\n",
        "            data *= np.sqrt(density) # sparse2 sparse rep.\n",
        "          else:\n",
        "            data *= np.sqrt(1 / density) / np.sqrt(n_components) if original else np.sqrt(1 / density) / np.sqrt(n_features)\n",
        "          values = torch.cat([values, data], dim=0)\n",
        "\n",
        "    if(sparse_type != None):\n",
        "      idx = torch.cat([row_idx.unsqueeze(0), col_idx.unsqueeze(0)], dim=0)\n",
        "      components = torch.sparse_coo_tensor(idx, values, size=(n_components, n_features), device=DEVICE) # sparse coo matrix\n",
        "      if(sparse_type == 'csc'): # representation conversion\n",
        "        components = components.to_sparse_csc()\n",
        "      elif(sparse_type == 'csr'):\n",
        "        components = components.to_sparse_csr()\n",
        "    else:\n",
        "      if(variation):\n",
        "        components *= np.sqrt(density) #sparse2 dense rep.\n",
        "      else:\n",
        "        components *= np.sqrt(1 / density) / np.sqrt(n_components) if original else np.sqrt(1 / density) / np.sqrt(n_features)\n",
        "\n",
        "  return components"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FC Layer with Random Projection"
      ],
      "metadata": {
        "id": "Do5hUMYSW_dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_Projected(nn.Module):\n",
        "  def __init__(self, input_size, output_size, d_units, rp_gen_algorithm='gaussian'):\n",
        "    super().__init__()\n",
        "    d = d_units\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.rp_gen_algorithm = rp_gen_algorithm\n",
        "    \n",
        "    self.theta_0 = torch.randn((input_size + 1) * output_size, device=DEVICE) / math.sqrt(input_size)\n",
        "\n",
        "    self.theta_d = nn.Parameter(torch.randn(d, device=DEVICE) / math.sqrt(d))\n",
        "\n",
        "    N = (input_size + 1) * output_size\n",
        "    M = d\n",
        "    \n",
        "    print(\"No. original params: \" + str(N))\n",
        "    print(\"No. params used through projection: \" + str(M))\n",
        "\n",
        "    if (rp_gen_algorithm == 'gaussian'):\n",
        "      self.P = generate_gaussian_rp(N,M).T\n",
        "    elif(rp_gen_algorithm == 'sparse'):\n",
        "      self.P = generate_sparse_rp(N,M)\n",
        "    elif(rp_gen_algorithm == 'sparse_coo'):\n",
        "      self.P = generate_sparse_rp(N,M, sparse_type='coo').T\n",
        "    elif(rp_gen_algorithm == 'sparse_csr'):\n",
        "      self.P = generate_sparse_rp(N,M, sparse_type='csr')\n",
        "    elif(rp_gen_algorithm == 'sparse_csc'):\n",
        "      self.P = generate_sparse_rp(N,M, sparse_type='csc')\n",
        "    elif(rp_gen_algorithm == 'sparse_original'):\n",
        "      self.P = generate_sparse_rp(N,M, original=True)\n",
        "    elif(rp_gen_algorithm == 'sparse2'):\n",
        "      self.P = generate_sparse_rp(N,M, variation=True, fullRange=False)\n",
        "    elif(rp_gen_algorithm == 'sparse2full'):\n",
        "      self.P = generate_sparse_rp(N,M, variation=True, fullRange=True)\n",
        "    elif(rp_gen_algorithm == 'sparse2_csc'):\n",
        "      self.P = generate_sparse_rp(N, M, variation=True, fullRange=False, sparse_type='csc')\n",
        "    elif(rp_gen_algorithm == 'sparse2full_csc'):\n",
        "      self.P = generate_sparse_rp(N, M, variation=True, fullRange=True, sparse_type='csc')\n",
        "    elif(rp_gen_algorithm == 'ortho'):\n",
        "      self.P = generate_gaussian_rp(N,M, ortho=True).T\n",
        "    else:\n",
        "      raise Exception(\"Supported random projections are: gaussian, sparse, sparse_coo, \" +\n",
        "                      \"sparse_csr, sparse_csc, sparse_original, sparse2, sparse2full, ortho\")\n",
        "\n",
        "  def forward(self, xb):\n",
        "    if(self.rp_gen_algorithm == 'sparse_coo'): # trick for mv multiplication pytorch support (vm mult doesn't work)\n",
        "      temp = self.theta_0 + self.P @ self.theta_d\n",
        "    else:\n",
        "      temp = self.theta_0 + self.theta_d @ self.P\n",
        "    t = xb @ (temp[:-self.output_size]).reshape(self.input_size, self.output_size)\n",
        "    res =  t + temp[-self.output_size:]\n",
        "    return res\n",
        "\n",
        "class Linear(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    return self.linear(xb)"
      ],
      "metadata": {
        "id": "MBrYLjT3XLhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3zS95EjJRSN"
      },
      "source": [
        "## Fully Connected Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "PlsS9NA0JvRP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rYTqYFLUKmM"
      },
      "outputs": [],
      "source": [
        "class ParamRegularizedMLP(BaseModel):\n",
        "    def __init__(self, input_size, output_size, num_mid_units, projected=True, d_units=[750, ], depth=3, rp_gen_algorithm='sparse_csc'):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = nn.ReLU()\n",
        "        \n",
        "        self.task=\"multiclass\"\n",
        "\n",
        "        if(projected):\n",
        "          if (depth < 2):\n",
        "            self.model = nn.Sequential(nn.Flatten(), Linear_Projected(input_size, output_size, d_units[0]))\n",
        "            return\n",
        "          \n",
        "          print(\"#layer_0\")\n",
        "          self.model = nn.Sequential(nn.Flatten(), Linear_Projected(input_size, num_mid_units, d_units[0], rp_gen_algorithm), self.activation)\n",
        "          for i in range(depth-2):\n",
        "            print(\"#layer_\" + str(i + 1))\n",
        "            \n",
        "            self.model.append(Linear_Projected(num_mid_units, num_mid_units, d_units[i+1 if len(d_units)>1 else 0], rp_gen_algorithm))\n",
        "            \n",
        "            self.model.append(self.activation)\n",
        "          \n",
        "          print(\"#layer_final\")\n",
        "          self.model.append(Linear_Projected(num_mid_units, output_size, d_units[-1 if len(d_units)>1 else 0], rp_gen_algorithm))\n",
        "        else:\n",
        "          if (depth < 2):\n",
        "            self.model = nn.Sequential(nn.Flatten(), Linear(input_size, output_size))\n",
        "            return\n",
        "          \n",
        "          self.model = nn.Sequential(nn.Flatten(), Linear(input_size, num_mid_units), self.activation)\n",
        "          for _ in range(depth-2):\n",
        "            self.model.append(Linear(num_mid_units, num_mid_units))\n",
        "            self.model.append(self.activation)\n",
        "          \n",
        "          self.model.append(Linear(num_mid_units, output_size))\n",
        "      \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), 1e-3)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "PuWq7OJrJ0Cp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a2JFzWJUNTS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918,
          "referenced_widgets": [
            "010d20e857304cc19a21364d4ec97025",
            "3da61addffbd491a9ad404de9cb2eec6",
            "01494264482943b58e6593d92a7f1e61",
            "9da6eff2689d4d34b79279bc92c149f9",
            "4b3503b802bd4320bb074778e33d20b2",
            "bdf3e2c31a7841a79327df12b19aacb9",
            "1616c666a5e74824bff3fe9ccb4027c0",
            "4aad7c16b52249e8bc9b5b45d517f943",
            "eae9ce4bbb3e4ae997137b07cb43ef02",
            "0fd4b86ac6924fbcb1d4206a5b9d21b6",
            "f936471903d94db08aea87a89cb0b2db",
            "62de767d96ed4806b1b066992b90fe67",
            "01fe8f73f6524cf086d4a297e9c63311",
            "9e0fe39e530e4987add1a37960eaf924",
            "26eafb012bb943068a1ef3a8390cbf96",
            "0724f2b33dc14114be3d6d424bcb9583",
            "c13965af253e4d1b9551cc724a746a07",
            "fc972ff412aa43c08ee5d26d370a7f86",
            "571eabf7327f44c988dfdb44f590f84f",
            "ae68f4752e5f45048e6858ff139bf653",
            "38c54fd71063441bb1e798f28faecd17",
            "001d74e9658f4e70ae74c5b2c3993fd7"
          ]
        },
        "outputId": "fae8e82e-f9b8-4192-e30e-7ab35071313c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#layer_0\n",
            "No. original params: 470600\n",
            "No. params used through projection: 2000\n",
            "#layer_final\n",
            "No. original params: 2010\n",
            "No. params used through projection: 1050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total VRAM: 15843721216\n",
            "Reserved VRAM: 7820279808\n",
            "Allocated VRAM: 3775467520\n",
            "Free VRAM: 4044812288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name       | Type       | Params\n",
            "------------------------------------------\n",
            "0 | activation | ReLU       | 0     \n",
            "1 | model      | Sequential | 3.1 K \n",
            "------------------------------------------\n",
            "3.1 K     Trainable params\n",
            "0         Non-trainable params\n",
            "3.1 K     Total params\n",
            "0.012     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "010d20e857304cc19a21364d4ec97025"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/b3e629b1971e1542/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "001d74e9658f4e70ae74c5b2c3993fd7"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/b3e629b1971e1542/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train_Allocated_VRAM</td><td>▁███████████████████████████████████████</td></tr><tr><td>Train_Free_VRAM</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train_Reserved_VRAM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train_Total_VRAM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>train_time_step</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train_Allocated_VRAM</td><td>3776123392</td></tr><tr><td>Train_Free_VRAM</td><td>13430272</td></tr><tr><td>Train_Reserved_VRAM</td><td>3789553664</td></tr><tr><td>Train_Total_VRAM</td><td>15843721216</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>train_acc</td><td>0.17188</td></tr><tr><td>train_loss</td><td>2.2121</td></tr><tr><td>train_time_step</td><td>15.2384</td></tr><tr><td>trainer/global_step</td><td>49</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">misty-dragon-43</strong> at: <a href=\"https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense/runs/amlenedw\" target=\"_blank\">https://wandb.ai/sapienza-ml/CIFAR-FC-SparseVsDense/runs/amlenedw</a><br/>Synced 5 W&B file(s), 21 media file(s), 9 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230121_134341-amlenedw/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if(\"fc\" in PROJECT_NAME.lower()):\n",
        "  model = ParamRegularizedMLP(784 * (1 if \"mnist\" in PROJECT_NAME.lower() else 3), 10, 200, True, [2000, 1050], 2, rp_gen_algorithm='ortho')\n",
        "\n",
        "  wandb_logger.watch(model, log=\"all\")\n",
        "\n",
        "  t = torch.cuda.get_device_properties(0).total_memory\n",
        "  r = torch.cuda.memory_reserved(0)\n",
        "  a = torch.cuda.memory_allocated(0)\n",
        "\n",
        "  print('Total VRAM:', t)\n",
        "  print('Reserved VRAM:', r)\n",
        "  print('Allocated VRAM:', a)\n",
        "  print('Free VRAM:', r-a)\n",
        "\n",
        "  trainer = pl.Trainer(max_epochs=30, accelerator=\"gpu\", logger=wandb_logger,\n",
        "                      callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
        "                                  LogPredictionsCallback(), checkpoint_callback])\n",
        "  trainer.fit(model, train_dataloaders=train_set, val_dataloaders=val_set)\n",
        "\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCIqbW07Dnb6"
      },
      "source": [
        "## Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "ymRTk0XfJ6HH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xDPQjN3DtV6"
      },
      "outputs": [],
      "source": [
        "class Conv2dProj(nn.Module):\n",
        "  def __init__(self, in_channels, kernel_size, out_channels, d_units, padding, stride, bias=True, rp_gen_algorithm='gaussian'):\n",
        "    super(Conv2dProj, self).__init__()\n",
        "\n",
        "    self.bias = bias\n",
        "\n",
        "    n_kernel_params = in_channels * kernel_size**2 * out_channels\n",
        "\n",
        "    if(bias):\n",
        "      N = n_kernel_params + out_channels\n",
        "    else:\n",
        "      N = n_kernel_params\n",
        "    \n",
        "    d = d_units\n",
        "    M = d\n",
        "    \n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.n_kernel_params = n_kernel_params\n",
        "    \n",
        "    self.theta_0 = torch.randn(N, device=DEVICE) / math.sqrt(N)\n",
        "\n",
        "    self.theta_d = nn.Parameter(torch.randn(d, device=DEVICE) / math.sqrt(d))\n",
        "    \n",
        "    print(\"No. original params: \" + str(N))\n",
        "    print(\"No. params used by projection: \" + str(d))\n",
        "\n",
        "    if (rp_gen_algorithm == 'gaussian'):\n",
        "      self.P = generate_gaussian_rp(N, M).T\n",
        "    elif(rp_gen_algorithm == 'sparse'):\n",
        "      self.P = generate_sparse_rp(N, M)\n",
        "    elif(rp_gen_algorithm == 'sparse_coo'):\n",
        "      self.P = generate_sparse_rp(N,M, sparse_type='coo').T\n",
        "    elif(rp_gen_algorithm == 'sparse_csr'):\n",
        "      self.P = generate_sparse_rp(N,M, sparse_type='csr')\n",
        "    elif(rp_gen_algorithm == 'sparse_csc'):\n",
        "      self.P = generate_sparse_rp(N,M, sparse_type='csc')\n",
        "    elif(rp_gen_algorithm == 'sparse_original'):\n",
        "      self.P = generate_sparse_rp(N, M, original=True)\n",
        "    elif(rp_gen_algorithm == 'sparse2'):\n",
        "      self.P = generate_sparse_rp(N, M, variation=True, fullRange=False)\n",
        "    elif(rp_gen_algorithm == 'sparse2full'):\n",
        "      self.P = generate_sparse_rp(N, M, variation=True, fullRange=True)\n",
        "    elif(rp_gen_algorithm == 'sparse2_csc'):\n",
        "      self.P = generate_sparse_rp(N, M, variation=True, fullRange=False, sparse_type='csc')\n",
        "    elif(rp_gen_algorithm == 'ortho'):\n",
        "      self.P = generate_gaussian_rp(N, M, True).T\n",
        "    else:\n",
        "      raise Exception(\"Supported random projections are: gaussian, sparse, sparse_coo, \" +\n",
        "                      \"sparse_csr, sparse_csc, sparse_original, sparse2, sparse2full, ortho\")\n",
        "\n",
        "    self.padding = padding\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, xb):\n",
        "    if (self.rp_gen_algorithm == 'sparse_coo'): # trick for mv multiplication pytorch support (vm mult doesn't work)\n",
        "      kernel = self.theta_0 +  self.P @ self.theta_d\n",
        "    else:\n",
        "      kernel = self.theta_0 + self.theta_d @ self.P\n",
        "    kernel_params = kernel[:self.n_kernel_params].reshape(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
        "\n",
        "    bias = kernel[self.n_kernel_params:] if self.bias else None\n",
        "    \n",
        "    res = torch.nn.functional.conv2d(xb, kernel_params, bias, stride=self.stride, padding=self.padding)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbk8S4QTHTw8"
      },
      "outputs": [],
      "source": [
        "class LeNet(BaseModel):\n",
        "    \n",
        "    def __init__(self, in_channels, output_size, projected=True, d_units=[4000, ], rp_gen_algorithm='gaussian'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.task = \"multiclass\"\n",
        "        self.output_size = output_size\n",
        "        if(projected):\n",
        "            multiple_values = len(d_units) > 1\n",
        "\n",
        "            print(\"#layer_fe_0\")\n",
        "            if (multiple_values):\n",
        "                self.model = nn.Sequential(Conv2dProj(in_channels, 5, 6, d_units[0], padding=2, stride=1, rp_gen_algorithm=rp_gen_algorithm), self.activation, self.pool)\n",
        "\n",
        "                print(\"#layer_fe_1\")\n",
        "                self.model.append(Conv2dProj(6, 5, 16, d_units[1], rp_gen_algorithm=rp_gen_algorithm, padding=0, stride=1))\n",
        "                self.model.append(self.activation)\n",
        "                self.model.append(self.pool)\n",
        "                self.model.append(nn.Flatten())\n",
        "\n",
        "                print(\"#layer_classifier_0\")\n",
        "                self.model.append(Linear_Projected(400, 120, d_units[2], rp_gen_algorithm=rp_gen_algorithm))\n",
        "                self.model.append(self.activation)\n",
        "\n",
        "                print(\"#layer_classifier_1\")\n",
        "                self.model.append(Linear_Projected(120, 84, d_units[3], rp_gen_algorithm=rp_gen_algorithm))\n",
        "                self.model.append(self.activation)\n",
        "\n",
        "                print(\"#layer_classifier_final\")\n",
        "                self.model.append(Linear_Projected(84, output_size, d_units[4], rp_gen_algorithm=rp_gen_algorithm))\n",
        "            else:\n",
        "                self.model = nn.Sequential(Conv2dProj(in_channels, 5, 6, d_units[0], padding=2, stride=1, rp_gen_algorithm=rp_gen_algorithm), self.activation, self.pool)\n",
        "\n",
        "                print(\"#layer_fe_1\")\n",
        "                self.model.append(Conv2dProj(6, 5, 16, d_units[0], rp_gen_algorithm=rp_gen_algorithm, padding=0, stride=1))\n",
        "                self.model.append(self.activation)\n",
        "                self.model.append(self.pool)\n",
        "                self.model.append(nn.Flatten())\n",
        "\n",
        "                print(\"#layer_classifier_0\")\n",
        "                self.model.append(Linear_Projected(400, 120, d_units[0], rp_gen_algorithm=rp_gen_algorithm))\n",
        "                self.model.append(self.activation)\n",
        "\n",
        "                print(\"#layer_classifier_1\")\n",
        "                self.model.append(Linear_Projected(120, 84, d_units[0], rp_gen_algorithm=rp_gen_algorithm))\n",
        "                self.model.append(self.activation)\n",
        "\n",
        "                print(\"#layer_classifier_final\")\n",
        "                self.model.append(Linear_Projected(84, output_size, d_units[0], rp_gen_algorithm=rp_gen_algorithm))\n",
        "        else:\n",
        "            self.model = nn.Sequential(nn.Conv2d(in_channels, 6, 5, padding=2, stride=1), self.activation, self.pool)\n",
        "\n",
        "            self.model.append(nn.Conv2d(6, 16, 5, padding=0, stride=1))\n",
        "            self.model.append(self.activation)\n",
        "            self.model.append(self.pool)\n",
        "            self.model.append(nn.Flatten())\n",
        "\n",
        "            self.model.append(nn.Linear(400, 120))\n",
        "            self.model.append(self.activation)\n",
        "\n",
        "            self.model.append(nn.Linear(120, 84))\n",
        "            self.model.append(self.activation)\n",
        "\n",
        "            self.model.append(nn.Linear(84, output_size))\n",
        "      \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), 1e-3)\n",
        "        #optimizer = torch.optim.SGD(self.parameters(), 1e-2, momentum=0.9)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(BaseModel):\n",
        "    def __init__(self, in_channels, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=1)\n",
        "        self.task = \"multiclass\"\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.model = nn.Sequential(nn.Conv2d(in_channels, 12, 5, padding=2, stride=2), self.activation, self.pool)\n",
        "        self.model.append(nn.Conv2d(12, 7, 7, padding=0, stride=2))\n",
        "        self.model.append(self.activation)\n",
        "        self.model.append(self.pool)\n",
        "        self.model.append(nn.Flatten())\n",
        "\n",
        "        self.model.append(nn.Linear(63, output_size))\n",
        "\n",
        "        self.n_parameters = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "        print(\"Num. parameters: \", self.n_parameters)\n",
        "      \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), 1e-3)\n",
        "        #optimizer = torch.optim.SGD(self.parameters(), 1e-2, momentum=0.9)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "hOYLH1nYND4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "cjCd048oJ7U5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M--k_QQSDvUV"
      },
      "outputs": [],
      "source": [
        "if(\"cnn\" in PROJECT_NAME.lower()):\n",
        "  model = LeNet(1 if \"mnist\" in PROJECT_NAME.lower() else 3, 10, True, [200,500,2000,500,300], rp_gen_algorithm=\"gaussian\")\n",
        "  #model = CNN(1 if \"mnist\" in PROJECT_NAME.lower() else 3, 10)\n",
        "  wandb_logger.watch(model, log=\"all\")\n",
        "\n",
        "  t = torch.cuda.get_device_properties(0).total_memory\n",
        "  r = torch.cuda.memory_reserved(0)\n",
        "  a = torch.cuda.memory_allocated(0)\n",
        "\n",
        "  print('Total VRAM:', t)\n",
        "  print('Reserved VRAM:', r)\n",
        "  print('Allocated VRAM:', a)\n",
        "  print('Free VRAM:', r-a)\n",
        "\n",
        "  trainer = pl.Trainer(max_epochs=100, accelerator=\"gpu\", logger=wandb_logger,\n",
        "                      callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
        "                                  LogPredictionsCallback(), checkpoint_callback])\n",
        "  trainer.fit(model, train_dataloaders=train_set, val_dataloaders=val_set)\n",
        "\n",
        "  wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "010d20e857304cc19a21364d4ec97025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3da61addffbd491a9ad404de9cb2eec6",
              "IPY_MODEL_01494264482943b58e6593d92a7f1e61",
              "IPY_MODEL_9da6eff2689d4d34b79279bc92c149f9"
            ],
            "layout": "IPY_MODEL_4b3503b802bd4320bb074778e33d20b2"
          }
        },
        "3da61addffbd491a9ad404de9cb2eec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf3e2c31a7841a79327df12b19aacb9",
            "placeholder": "​",
            "style": "IPY_MODEL_1616c666a5e74824bff3fe9ccb4027c0",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "01494264482943b58e6593d92a7f1e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aad7c16b52249e8bc9b5b45d517f943",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eae9ce4bbb3e4ae997137b07cb43ef02",
            "value": 2
          }
        },
        "9da6eff2689d4d34b79279bc92c149f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd4b86ac6924fbcb1d4206a5b9d21b6",
            "placeholder": "​",
            "style": "IPY_MODEL_f936471903d94db08aea87a89cb0b2db",
            "value": " 2/2 [00:00&lt;00:00,  2.65it/s]"
          }
        },
        "4b3503b802bd4320bb074778e33d20b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "bdf3e2c31a7841a79327df12b19aacb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1616c666a5e74824bff3fe9ccb4027c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aad7c16b52249e8bc9b5b45d517f943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eae9ce4bbb3e4ae997137b07cb43ef02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fd4b86ac6924fbcb1d4206a5b9d21b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f936471903d94db08aea87a89cb0b2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62de767d96ed4806b1b066992b90fe67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01fe8f73f6524cf086d4a297e9c63311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e0fe39e530e4987add1a37960eaf924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26eafb012bb943068a1ef3a8390cbf96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0724f2b33dc14114be3d6d424bcb9583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13965af253e4d1b9551cc724a746a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc972ff412aa43c08ee5d26d370a7f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e0fe39e530e4987add1a37960eaf924",
            "placeholder": "​",
            "style": "IPY_MODEL_26eafb012bb943068a1ef3a8390cbf96",
            "value": "Epoch 0:   9%"
          }
        },
        "571eabf7327f44c988dfdb44f590f84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62de767d96ed4806b1b066992b90fe67",
            "max": 782,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01fe8f73f6524cf086d4a297e9c63311",
            "value": 67
          }
        },
        "ae68f4752e5f45048e6858ff139bf653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0724f2b33dc14114be3d6d424bcb9583",
            "placeholder": "​",
            "style": "IPY_MODEL_c13965af253e4d1b9551cc724a746a07",
            "value": " 67/782 [00:03&lt;00:32, 21.69it/s, loss=2.17, v_num=nedw]"
          }
        },
        "38c54fd71063441bb1e798f28faecd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "001d74e9658f4e70ae74c5b2c3993fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc972ff412aa43c08ee5d26d370a7f86",
              "IPY_MODEL_571eabf7327f44c988dfdb44f590f84f",
              "IPY_MODEL_ae68f4752e5f45048e6858ff139bf653"
            ],
            "layout": "IPY_MODEL_38c54fd71063441bb1e798f28faecd17"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}